{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from simple_convnet import SimpleConvNet\n",
    "from common.trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# 시간이 오래 걸릴 경우 데이터를 줄인다.\n",
    "x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "x_test, t_test = x_test[:1000], t_test[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.299855736830102\n",
      "=== epoch:1, train acc:0.123, test acc:0.141 ===\n",
      "train loss:2.297038028496428\n",
      "train loss:2.2924849410776904\n",
      "train loss:2.2888870270727795\n",
      "train loss:2.2781410868783767\n",
      "train loss:2.2645470040386186\n",
      "train loss:2.2479880894924658\n",
      "train loss:2.233550457810495\n",
      "train loss:2.1980582048062556\n",
      "train loss:2.226021805980702\n",
      "train loss:2.1531566686529584\n",
      "train loss:2.1068859007022045\n",
      "train loss:2.0622498482583316\n",
      "train loss:2.0633346496758036\n",
      "train loss:2.009715839390204\n",
      "train loss:1.9013166762421962\n",
      "train loss:1.9165607219190393\n",
      "train loss:1.7789849086810374\n",
      "train loss:1.677295912396616\n",
      "train loss:1.6142354963427072\n",
      "train loss:1.5096699154446305\n",
      "train loss:1.5151228748233343\n",
      "train loss:1.4448525808917763\n",
      "train loss:1.2853482810130001\n",
      "train loss:1.2122860041908612\n",
      "train loss:1.096644123041051\n",
      "train loss:1.2070363551436467\n",
      "train loss:1.1835292049460837\n",
      "train loss:1.0458491989847376\n",
      "train loss:0.9632056353130983\n",
      "train loss:0.9834428918495631\n",
      "train loss:0.9125903867906894\n",
      "train loss:0.8535741243525915\n",
      "train loss:0.6240919122559968\n",
      "train loss:0.773963489270443\n",
      "train loss:0.7085286424406835\n",
      "train loss:0.7048236988493154\n",
      "train loss:0.6497366154987203\n",
      "train loss:0.5312446794872596\n",
      "train loss:0.7725924480794815\n",
      "train loss:0.654954579883289\n",
      "train loss:0.5514140326959658\n",
      "train loss:0.5223716318940967\n",
      "train loss:0.5352635599118385\n",
      "train loss:0.4698293716288424\n",
      "train loss:0.5635530220424149\n",
      "train loss:0.6361630273416102\n",
      "train loss:0.5980975517523549\n",
      "train loss:0.3331262102168688\n",
      "train loss:0.4241132712808499\n",
      "train loss:0.5415943913581603\n",
      "=== epoch:2, train acc:0.798, test acc:0.789 ===\n",
      "train loss:0.4373683802918463\n",
      "train loss:0.5427473939947077\n",
      "train loss:0.4298865414933748\n",
      "train loss:0.6184310302898692\n",
      "train loss:0.4783899460241696\n",
      "train loss:0.5442584911795627\n",
      "train loss:0.347782700908588\n",
      "train loss:0.4776789343298728\n",
      "train loss:0.5951897448789868\n",
      "train loss:0.515104998906343\n",
      "train loss:0.5087914643660079\n",
      "train loss:0.4235633502712799\n",
      "train loss:0.48483357767838575\n",
      "train loss:0.6641071254610246\n",
      "train loss:0.5570668980841486\n",
      "train loss:0.41142463751672337\n",
      "train loss:0.4284532696785014\n",
      "train loss:0.4336958305216282\n",
      "train loss:0.47257589818951795\n",
      "train loss:0.4981927240854157\n",
      "train loss:0.4860245306583974\n",
      "train loss:0.40886688385174147\n",
      "train loss:0.4297695521307825\n",
      "train loss:0.43943745520199856\n",
      "train loss:0.5158284745176769\n",
      "train loss:0.3922044482921656\n",
      "train loss:0.31775587294525853\n",
      "train loss:0.3277404662833623\n",
      "train loss:0.2557347130825741\n",
      "train loss:0.5355706672300069\n",
      "train loss:0.40774801409035477\n",
      "train loss:0.34976375950251537\n",
      "train loss:0.3078651909215006\n",
      "train loss:0.28802757698429654\n",
      "train loss:0.30559470822996315\n",
      "train loss:0.5716251781513444\n",
      "train loss:0.27403025634381334\n",
      "train loss:0.3143141021366214\n",
      "train loss:0.4759116610045517\n",
      "train loss:0.33684858745028395\n",
      "train loss:0.3587789380432376\n",
      "train loss:0.5037417351198262\n",
      "train loss:0.26250524070852593\n",
      "train loss:0.36488724249295434\n",
      "train loss:0.24600925681600183\n",
      "train loss:0.3570319250441281\n",
      "train loss:0.34401406743781715\n",
      "train loss:0.24237737556496497\n",
      "train loss:0.26923300037391196\n",
      "train loss:0.26533482502812367\n",
      "=== epoch:3, train acc:0.883, test acc:0.863 ===\n",
      "train loss:0.2953783898010759\n",
      "train loss:0.27873649806464956\n",
      "train loss:0.23957169305140036\n",
      "train loss:0.2939659557399691\n",
      "train loss:0.320872521449148\n",
      "train loss:0.4481231049613283\n",
      "train loss:0.3599093038435957\n",
      "train loss:0.34737630664085545\n",
      "train loss:0.4012802729619333\n",
      "train loss:0.35437961338045115\n",
      "train loss:0.23975939851164935\n",
      "train loss:0.2742910518933763\n",
      "train loss:0.30197828169677143\n",
      "train loss:0.3539790399761729\n",
      "train loss:0.31645003172548497\n",
      "train loss:0.24734653287651384\n",
      "train loss:0.2912080577440452\n",
      "train loss:0.3741498057594579\n",
      "train loss:0.2974772317380554\n",
      "train loss:0.3156799225654042\n",
      "train loss:0.2096701189529168\n",
      "train loss:0.25258927352498445\n",
      "train loss:0.45622413955913904\n",
      "train loss:0.2012487673705589\n",
      "train loss:0.19652579441416468\n",
      "train loss:0.26762185766498875\n",
      "train loss:0.31156097022065343\n",
      "train loss:0.39607546249343883\n",
      "train loss:0.17543369958371163\n",
      "train loss:0.4500119797844821\n",
      "train loss:0.3307852341796528\n",
      "train loss:0.19741345312336797\n",
      "train loss:0.33275592376473717\n",
      "train loss:0.16810469330443287\n",
      "train loss:0.3042529156733009\n",
      "train loss:0.29451752745048\n",
      "train loss:0.18385569600561888\n",
      "train loss:0.36600399234373404\n",
      "train loss:0.2826299777735039\n",
      "train loss:0.28412639204043244\n",
      "train loss:0.36780414734199374\n",
      "train loss:0.2476678951826004\n",
      "train loss:0.3517906869623698\n",
      "train loss:0.2545251895727718\n",
      "train loss:0.42447395125382414\n",
      "train loss:0.35426703299171997\n",
      "train loss:0.16839992440316628\n",
      "train loss:0.2522619907389267\n",
      "train loss:0.4392304352530344\n",
      "train loss:0.30341514951446447\n",
      "=== epoch:4, train acc:0.908, test acc:0.884 ===\n",
      "train loss:0.21304937572261562\n",
      "train loss:0.3800508536174222\n",
      "train loss:0.24216572931522126\n",
      "train loss:0.2608865212356459\n",
      "train loss:0.4473058441389026\n",
      "train loss:0.319531017510897\n",
      "train loss:0.3254442797221403\n",
      "train loss:0.3977235058578795\n",
      "train loss:0.2829441862601633\n",
      "train loss:0.40083290721976467\n",
      "train loss:0.18843366770843215\n",
      "train loss:0.21839626709866547\n",
      "train loss:0.2346220081673213\n",
      "train loss:0.14723682568288443\n",
      "train loss:0.3108990268735557\n",
      "train loss:0.21039771915935773\n",
      "train loss:0.21771340926817506\n",
      "train loss:0.35544656259896434\n",
      "train loss:0.2231451054137732\n",
      "train loss:0.32798030647136067\n",
      "train loss:0.33024401109800894\n",
      "train loss:0.25747356508580005\n",
      "train loss:0.2961271764041501\n",
      "train loss:0.23057761892174294\n",
      "train loss:0.13751187137174165\n",
      "train loss:0.17490959760239483\n",
      "train loss:0.23038913989539866\n",
      "train loss:0.3372654957382963\n",
      "train loss:0.17898324987943148\n",
      "train loss:0.21916101759101095\n",
      "train loss:0.2825982970267795\n",
      "train loss:0.2695726913814478\n",
      "train loss:0.34798787561815486\n",
      "train loss:0.13286644241276963\n",
      "train loss:0.182657926869191\n",
      "train loss:0.23088577896190593\n",
      "train loss:0.2036402193678726\n",
      "train loss:0.2345273918882776\n",
      "train loss:0.2124912973278969\n",
      "train loss:0.22122250839797822\n",
      "train loss:0.15024137652370062\n",
      "train loss:0.19542457059059629\n",
      "train loss:0.17820961780526776\n",
      "train loss:0.18000639778313854\n",
      "train loss:0.19466847858545958\n",
      "train loss:0.2195796924255213\n",
      "train loss:0.43505285858585396\n",
      "train loss:0.17213463515193905\n",
      "train loss:0.34306079281810853\n",
      "train loss:0.14129540257729659\n",
      "=== epoch:5, train acc:0.916, test acc:0.892 ===\n",
      "train loss:0.19686004148417852\n",
      "train loss:0.4628703141365358\n",
      "train loss:0.16569982301135966\n",
      "train loss:0.4337619466984385\n",
      "train loss:0.3319081182296685\n",
      "train loss:0.21881332013400762\n",
      "train loss:0.1964063834943786\n",
      "train loss:0.21785657369012335\n",
      "train loss:0.23203419884092075\n",
      "train loss:0.28285281129498663\n",
      "train loss:0.27887323467967845\n",
      "train loss:0.18269870307268982\n",
      "train loss:0.20102277944951047\n",
      "train loss:0.17327459527773142\n",
      "train loss:0.2335557983516778\n",
      "train loss:0.34280199756153606\n",
      "train loss:0.2383419235058516\n",
      "train loss:0.27593622560639913\n",
      "train loss:0.2614907020736396\n",
      "train loss:0.15652550544022195\n",
      "train loss:0.3754654601323923\n",
      "train loss:0.23160643423280888\n",
      "train loss:0.2514689208942137\n",
      "train loss:0.18472455923641143\n",
      "train loss:0.14000860468420076\n",
      "train loss:0.2189059434787536\n",
      "train loss:0.2939093087434882\n",
      "train loss:0.2590650552914285\n",
      "train loss:0.20637150442806654\n",
      "train loss:0.14686707496304524\n",
      "train loss:0.22620795509884026\n",
      "train loss:0.23717776020292242\n",
      "train loss:0.2459951289066401\n",
      "train loss:0.270720366756957\n",
      "train loss:0.16144534866959198\n",
      "train loss:0.127211037299094\n",
      "train loss:0.16151059025287715\n",
      "train loss:0.1660996579719151\n",
      "train loss:0.18765983156844876\n",
      "train loss:0.15349286093464395\n",
      "train loss:0.1939786933348625\n",
      "train loss:0.40336814227974194\n",
      "train loss:0.22292841321772477\n",
      "train loss:0.19998861630628376\n",
      "train loss:0.23967284577013934\n",
      "train loss:0.2243021340015635\n",
      "train loss:0.1968850445880523\n",
      "train loss:0.23675165180186547\n",
      "train loss:0.23084896434462274\n",
      "train loss:0.07648190938643029\n",
      "=== epoch:6, train acc:0.919, test acc:0.904 ===\n",
      "train loss:0.29656217170274557\n",
      "train loss:0.183211355260059\n",
      "train loss:0.22274321195754926\n",
      "train loss:0.22066654371395106\n",
      "train loss:0.2651835929225759\n",
      "train loss:0.1959249178129054\n",
      "train loss:0.26507600041120427\n",
      "train loss:0.30581110958225716\n",
      "train loss:0.1327354364669741\n",
      "train loss:0.18829302133963954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.30899100320909034\n",
      "train loss:0.17624581481253826\n",
      "train loss:0.10633960477170909\n",
      "train loss:0.2725611296986012\n",
      "train loss:0.17292860902569157\n",
      "train loss:0.19684684621973914\n",
      "train loss:0.219841354385138\n",
      "train loss:0.2586668459837135\n",
      "train loss:0.14912073881371601\n",
      "train loss:0.18200281256053763\n",
      "train loss:0.1865711109081769\n",
      "train loss:0.14832571540944262\n",
      "train loss:0.2170008287417186\n",
      "train loss:0.23595178026436214\n",
      "train loss:0.18309100355010932\n",
      "train loss:0.11486768841373873\n",
      "train loss:0.21238677518662955\n",
      "train loss:0.20876550925162923\n",
      "train loss:0.17137433727504015\n",
      "train loss:0.1808128055509513\n",
      "train loss:0.20284721939008968\n",
      "train loss:0.149553914004652\n",
      "train loss:0.1752619319951922\n",
      "train loss:0.07772432782233124\n",
      "train loss:0.13386793946320974\n",
      "train loss:0.1521283120616418\n",
      "train loss:0.1602691924838912\n",
      "train loss:0.1757419210549897\n",
      "train loss:0.13127427364700023\n",
      "train loss:0.1522723348080762\n",
      "train loss:0.11347140969413663\n",
      "train loss:0.1293163999186188\n",
      "train loss:0.2069234321665079\n",
      "train loss:0.21194654410622593\n",
      "train loss:0.22163049889870037\n",
      "train loss:0.11211168788718667\n",
      "train loss:0.11179519604329453\n",
      "train loss:0.15579273009911515\n",
      "train loss:0.12798815872352137\n",
      "train loss:0.12568469035914634\n",
      "=== epoch:7, train acc:0.938, test acc:0.917 ===\n",
      "train loss:0.10672712832757454\n",
      "train loss:0.21127327661328457\n",
      "train loss:0.11646526024941284\n",
      "train loss:0.15094131026714735\n",
      "train loss:0.19479340191650163\n",
      "train loss:0.25590488085233953\n",
      "train loss:0.13153340263109237\n",
      "train loss:0.12652779761108301\n",
      "train loss:0.18080174362001425\n",
      "train loss:0.07668353929236087\n",
      "train loss:0.07641172856036905\n",
      "train loss:0.13929509644706986\n",
      "train loss:0.18914128692170934\n",
      "train loss:0.4414305805239108\n",
      "train loss:0.27183234865857964\n",
      "train loss:0.21374243345699764\n",
      "train loss:0.16955948252761274\n",
      "train loss:0.14017180317551808\n",
      "train loss:0.1081420858093707\n",
      "train loss:0.07813906261880033\n",
      "train loss:0.16412672835061792\n",
      "train loss:0.20337784314707213\n",
      "train loss:0.20347103190586638\n",
      "train loss:0.11085854082872837\n",
      "train loss:0.295788565299431\n",
      "train loss:0.08428006245867353\n",
      "train loss:0.14810860920511856\n",
      "train loss:0.12577388629481764\n",
      "train loss:0.16022641155382933\n",
      "train loss:0.09407970505457962\n",
      "train loss:0.11354100768131975\n",
      "train loss:0.16944199843519525\n",
      "train loss:0.2289574251315543\n",
      "train loss:0.15408004743900913\n",
      "train loss:0.2593780510907212\n",
      "train loss:0.24520478739333765\n",
      "train loss:0.11925240039855935\n",
      "train loss:0.12916303767356038\n",
      "train loss:0.2288779182171605\n",
      "train loss:0.2071226388122223\n",
      "train loss:0.09183729624836136\n",
      "train loss:0.20804875836969697\n",
      "train loss:0.13200007391245155\n",
      "train loss:0.07291534818970001\n",
      "train loss:0.15037369040551338\n",
      "train loss:0.12125019591212921\n",
      "train loss:0.16016856001186353\n",
      "train loss:0.060770037108491805\n",
      "train loss:0.19921311843121753\n",
      "train loss:0.20105577818969858\n",
      "=== epoch:8, train acc:0.952, test acc:0.923 ===\n",
      "train loss:0.1529879518911522\n",
      "train loss:0.042399962909056475\n",
      "train loss:0.1560334100229537\n",
      "train loss:0.2561641272995775\n",
      "train loss:0.17700982561399276\n",
      "train loss:0.14966986774252233\n",
      "train loss:0.10139293699588\n",
      "train loss:0.17458731027166202\n",
      "train loss:0.14847042040128114\n",
      "train loss:0.10105291653904869\n",
      "train loss:0.09314864818576878\n",
      "train loss:0.11465886134407725\n",
      "train loss:0.2238294724849913\n",
      "train loss:0.07506547371921261\n",
      "train loss:0.13644930354468127\n",
      "train loss:0.14536246916828785\n",
      "train loss:0.18893952154995808\n",
      "train loss:0.10716944728350797\n",
      "train loss:0.17007228271758795\n",
      "train loss:0.15219518079308267\n",
      "train loss:0.18525685059379188\n",
      "train loss:0.11267685492377776\n",
      "train loss:0.12922797234782832\n",
      "train loss:0.1535702841002556\n",
      "train loss:0.0805376960495618\n",
      "train loss:0.13477425379268718\n",
      "train loss:0.1173365343618323\n",
      "train loss:0.11605887965864632\n",
      "train loss:0.22194860897787316\n",
      "train loss:0.1289859314206126\n",
      "train loss:0.1368008890899801\n",
      "train loss:0.12869075761527135\n",
      "train loss:0.106372734057865\n",
      "train loss:0.14689994146368315\n",
      "train loss:0.11966774759806581\n",
      "train loss:0.10054098922325315\n",
      "train loss:0.05692736603783621\n",
      "train loss:0.12381345240828201\n",
      "train loss:0.12482917415269419\n",
      "train loss:0.15824345349074587\n",
      "train loss:0.07593249255795052\n",
      "train loss:0.19066155336626017\n",
      "train loss:0.2372860656988447\n",
      "train loss:0.07007672490986223\n",
      "train loss:0.13352443769282785\n",
      "train loss:0.052616135577586355\n",
      "train loss:0.08579246852379978\n",
      "train loss:0.07738535729379871\n",
      "train loss:0.10380295763808789\n",
      "train loss:0.08062969880797889\n",
      "=== epoch:9, train acc:0.953, test acc:0.919 ===\n",
      "train loss:0.3595187738971894\n",
      "train loss:0.049732896192598366\n",
      "train loss:0.09936812077377973\n",
      "train loss:0.18609733158794042\n",
      "train loss:0.06985091444894789\n",
      "train loss:0.07016947012926171\n",
      "train loss:0.11514686834358423\n",
      "train loss:0.08684723909684175\n",
      "train loss:0.11832105079662497\n",
      "train loss:0.05312305985438384\n",
      "train loss:0.10052560807432213\n",
      "train loss:0.22958327496681388\n",
      "train loss:0.0457265926173673\n",
      "train loss:0.09814669572765836\n",
      "train loss:0.08096161042257584\n",
      "train loss:0.23784385718882523\n",
      "train loss:0.061317445284634765\n",
      "train loss:0.04992343010210913\n",
      "train loss:0.1561406009820876\n",
      "train loss:0.10289323381549248\n",
      "train loss:0.07347236778233972\n",
      "train loss:0.07756528897821748\n",
      "train loss:0.1564396505065829\n",
      "train loss:0.08692812200078694\n",
      "train loss:0.09936333981728299\n",
      "train loss:0.09136711267739969\n",
      "train loss:0.10218773926751316\n",
      "train loss:0.10530354483353507\n",
      "train loss:0.1132198977782364\n",
      "train loss:0.12370840223889744\n",
      "train loss:0.09215869826872135\n",
      "train loss:0.1338590310702288\n",
      "train loss:0.1212579056472404\n",
      "train loss:0.08420215767513342\n",
      "train loss:0.09830606495233962\n",
      "train loss:0.13695510511295658\n",
      "train loss:0.19380296500388056\n",
      "train loss:0.16684213299833392\n",
      "train loss:0.06295609230121985\n",
      "train loss:0.07396444923934664\n",
      "train loss:0.1441153234258282\n",
      "train loss:0.1320768141816405\n",
      "train loss:0.20211440033178663\n",
      "train loss:0.07490519269461861\n",
      "train loss:0.09258830875025445\n",
      "train loss:0.09873020080440238\n",
      "train loss:0.13269875577547022\n",
      "train loss:0.1577595426561588\n",
      "train loss:0.07005550044414949\n",
      "train loss:0.15711521915676557\n",
      "=== epoch:10, train acc:0.959, test acc:0.924 ===\n",
      "train loss:0.12157865333862171\n",
      "train loss:0.17826556754073775\n",
      "train loss:0.18164904533176976\n",
      "train loss:0.04486617846484972\n",
      "train loss:0.09351056721147173\n",
      "train loss:0.05376278430332885\n",
      "train loss:0.09164503031360269\n",
      "train loss:0.13754302867862658\n",
      "train loss:0.11707406539156903\n",
      "train loss:0.08364068544946397\n",
      "train loss:0.1045042135270788\n",
      "train loss:0.06278639598777161\n",
      "train loss:0.06988128649327395\n",
      "train loss:0.07349347806247225\n",
      "train loss:0.06534684396524422\n",
      "train loss:0.07658100552330244\n",
      "train loss:0.13275776993162036\n",
      "train loss:0.07220752501799117\n",
      "train loss:0.11546250457804927\n",
      "train loss:0.05255173631655711\n",
      "train loss:0.14305168202016266\n",
      "train loss:0.16855661740103983\n",
      "train loss:0.09401928935092632\n",
      "train loss:0.12960288250343516\n",
      "train loss:0.10469603409419102\n",
      "train loss:0.07460345378937962\n",
      "train loss:0.08834913490232972\n",
      "train loss:0.07095319747307402\n",
      "train loss:0.09778999918011243\n",
      "train loss:0.03781311999354048\n",
      "train loss:0.15886777855275644\n",
      "train loss:0.1336710668055386\n",
      "train loss:0.19982602252495524\n",
      "train loss:0.11813968080784233\n",
      "train loss:0.13516766088701254\n",
      "train loss:0.10713583040308805\n",
      "train loss:0.1660087463260294\n",
      "train loss:0.03665058091643126\n",
      "train loss:0.06276003366309685\n",
      "train loss:0.11341446968318128\n",
      "train loss:0.11358061752848023\n",
      "train loss:0.08990708528208742\n",
      "train loss:0.17395717050925863\n",
      "train loss:0.06685003538864259\n",
      "train loss:0.06441245581588852\n",
      "train loss:0.08400181458532555\n",
      "train loss:0.07870814509068175\n",
      "train loss:0.10813558691609333\n",
      "train loss:0.05357670265325686\n",
      "train loss:0.07467230074766651\n",
      "=== epoch:11, train acc:0.966, test acc:0.939 ===\n",
      "train loss:0.07646899976538968\n",
      "train loss:0.051204673996215117\n",
      "train loss:0.05747744916428201\n",
      "train loss:0.04564318435352881\n",
      "train loss:0.09480317322137609\n",
      "train loss:0.13255491318344342\n",
      "train loss:0.03560034236560227\n",
      "train loss:0.04286123681971933\n",
      "train loss:0.11179909438226282\n",
      "train loss:0.05047795447256502\n",
      "train loss:0.04778049949348792\n",
      "train loss:0.07395776590796534\n",
      "train loss:0.09585678350345025\n",
      "train loss:0.04650273898623295\n",
      "train loss:0.05801913868524829\n",
      "train loss:0.0767359683997626\n",
      "train loss:0.0777172184612441\n",
      "train loss:0.08310387283404476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.13211012642196202\n",
      "train loss:0.09146995331320994\n",
      "train loss:0.055986256236100054\n",
      "train loss:0.0836003892702771\n",
      "train loss:0.09887340894675618\n",
      "train loss:0.10979541717739474\n",
      "train loss:0.10167388653472761\n",
      "train loss:0.10036008322635913\n",
      "train loss:0.11723899306160958\n",
      "train loss:0.06173546517154786\n",
      "train loss:0.07492856178528315\n",
      "train loss:0.049612584379668485\n",
      "train loss:0.08785933754979369\n",
      "train loss:0.027768187343866216\n",
      "train loss:0.11561212211887088\n",
      "train loss:0.06540259124073007\n",
      "train loss:0.1079662632427932\n",
      "train loss:0.09159968489952053\n",
      "train loss:0.04278697806757716\n",
      "train loss:0.05312710612904778\n",
      "train loss:0.06627679196758968\n",
      "train loss:0.11254417148056907\n",
      "train loss:0.08121935536494236\n",
      "train loss:0.04062317035338633\n",
      "train loss:0.1659635000300755\n",
      "train loss:0.08798812834774385\n",
      "train loss:0.13683242956013456\n",
      "train loss:0.058010535337185386\n",
      "train loss:0.03248811843362299\n",
      "train loss:0.11104235249239755\n",
      "train loss:0.05020940830118631\n",
      "train loss:0.15358420731509503\n",
      "=== epoch:12, train acc:0.974, test acc:0.947 ===\n",
      "train loss:0.05064108283010545\n",
      "train loss:0.03611684357729468\n",
      "train loss:0.04752426970355662\n",
      "train loss:0.11249060641984716\n",
      "train loss:0.08519372661012035\n",
      "train loss:0.11155476134558563\n",
      "train loss:0.15874314703055425\n",
      "train loss:0.09281283891910061\n",
      "train loss:0.07047810913726123\n",
      "train loss:0.1419226516874456\n",
      "train loss:0.08151198604355532\n",
      "train loss:0.018416974811517182\n",
      "train loss:0.04839217030814345\n",
      "train loss:0.1018480476604613\n",
      "train loss:0.12193558459734412\n",
      "train loss:0.050773017058048264\n",
      "train loss:0.06704637705289533\n",
      "train loss:0.08945228167779427\n",
      "train loss:0.08207052267503055\n",
      "train loss:0.16474766893784365\n",
      "train loss:0.0992755141129143\n",
      "train loss:0.0658706514181481\n",
      "train loss:0.0724611537052838\n",
      "train loss:0.060683487458176394\n",
      "train loss:0.07746054188596908\n",
      "train loss:0.12592759021657207\n",
      "train loss:0.08291883805416597\n",
      "train loss:0.05691702149392357\n",
      "train loss:0.05528258161171589\n",
      "train loss:0.12163454779244157\n",
      "train loss:0.07066586737267967\n",
      "train loss:0.035675477729574266\n",
      "train loss:0.048175230666261955\n",
      "train loss:0.07782725958606523\n",
      "train loss:0.04711801513649606\n",
      "train loss:0.07402306886242158\n",
      "train loss:0.057173713547095384\n",
      "train loss:0.06200750613313727\n",
      "train loss:0.03224252862119399\n",
      "train loss:0.08652089246874331\n",
      "train loss:0.06809073334181788\n",
      "train loss:0.042398164585226265\n",
      "train loss:0.11266163622514917\n",
      "train loss:0.07988781333106401\n",
      "train loss:0.05502027750506432\n",
      "train loss:0.04234239228521353\n",
      "train loss:0.08121142134757545\n",
      "train loss:0.07423305580644404\n",
      "train loss:0.035996905895798424\n",
      "train loss:0.023135909492183984\n",
      "=== epoch:13, train acc:0.971, test acc:0.948 ===\n",
      "train loss:0.07009823606872478\n",
      "train loss:0.05293508168597787\n",
      "train loss:0.0608696379657902\n",
      "train loss:0.023625477301884544\n",
      "train loss:0.06283325851297003\n",
      "train loss:0.17456988224914505\n",
      "train loss:0.04380772853305455\n",
      "train loss:0.04900122652666262\n",
      "train loss:0.05618480384646514\n",
      "train loss:0.06960475531755508\n",
      "train loss:0.07449967722816753\n",
      "train loss:0.0454997860098622\n",
      "train loss:0.04579597264650694\n",
      "train loss:0.06729776354933682\n",
      "train loss:0.04221974990743317\n",
      "train loss:0.06973745091037233\n",
      "train loss:0.04273807562185279\n",
      "train loss:0.046082904256570814\n",
      "train loss:0.05040386678614895\n",
      "train loss:0.061696496745910955\n",
      "train loss:0.03903592242094589\n",
      "train loss:0.050663653664538046\n",
      "train loss:0.059834880935337065\n",
      "train loss:0.06442348976837875\n",
      "train loss:0.04633671399312382\n",
      "train loss:0.031236282549402486\n",
      "train loss:0.11202109953372462\n",
      "train loss:0.04376646376984701\n",
      "train loss:0.03467323246729869\n",
      "train loss:0.050716535386380185\n",
      "train loss:0.03452261214416113\n",
      "train loss:0.08474893239511351\n",
      "train loss:0.027176076831329582\n",
      "train loss:0.05660076571241865\n",
      "train loss:0.08690085469744732\n",
      "train loss:0.03839929331657803\n",
      "train loss:0.09390860373372854\n",
      "train loss:0.05555497744552941\n",
      "train loss:0.15866784871487147\n",
      "train loss:0.0278624884777268\n",
      "train loss:0.053012240144810194\n",
      "train loss:0.06786358081879976\n",
      "train loss:0.06524232306971096\n",
      "train loss:0.019306602422976243\n",
      "train loss:0.033815443103921206\n",
      "train loss:0.030826689144854758\n",
      "train loss:0.05516097885916029\n",
      "train loss:0.07860286629698064\n",
      "train loss:0.04512825759836733\n",
      "train loss:0.0585627590776266\n",
      "=== epoch:14, train acc:0.98, test acc:0.95 ===\n",
      "train loss:0.06402689033888569\n",
      "train loss:0.03468841137898126\n",
      "train loss:0.06962301463542722\n",
      "train loss:0.03694733183093226\n",
      "train loss:0.060544011906765845\n",
      "train loss:0.0681767352818086\n",
      "train loss:0.033036431335283535\n",
      "train loss:0.027632303880172914\n",
      "train loss:0.05750403899398461\n",
      "train loss:0.11069104152238184\n",
      "train loss:0.056118452043919215\n",
      "train loss:0.04076479965322911\n",
      "train loss:0.033621577532206286\n",
      "train loss:0.038251697652758286\n",
      "train loss:0.04321161864830168\n",
      "train loss:0.03516818205726055\n",
      "train loss:0.0331623176151626\n",
      "train loss:0.14446093188343545\n",
      "train loss:0.1945322226044359\n",
      "train loss:0.08567301156311513\n",
      "train loss:0.12446866610844856\n",
      "train loss:0.030722148265638367\n",
      "train loss:0.05373074011356744\n",
      "train loss:0.03498544326019071\n",
      "train loss:0.0572212018306965\n",
      "train loss:0.04840026341150249\n",
      "train loss:0.04175005684096975\n",
      "train loss:0.0720905664292783\n",
      "train loss:0.06445685186241445\n",
      "train loss:0.025653895656991613\n",
      "train loss:0.11475908679692269\n",
      "train loss:0.03926342934972569\n",
      "train loss:0.04658460044501512\n",
      "train loss:0.054109106299241205\n",
      "train loss:0.07781430384650304\n",
      "train loss:0.06313138168299877\n",
      "train loss:0.027466446424772674\n",
      "train loss:0.048771304823765835\n",
      "train loss:0.0822779802766648\n",
      "train loss:0.024328647960257875\n",
      "train loss:0.07691326964014117\n",
      "train loss:0.02787952440569087\n",
      "train loss:0.08768384013547582\n",
      "train loss:0.021571528345826464\n",
      "train loss:0.04165067954495573\n",
      "train loss:0.050042710156572016\n",
      "train loss:0.0811014992523394\n",
      "train loss:0.031236368450101023\n",
      "train loss:0.10601231455978985\n",
      "train loss:0.11491624763547759\n",
      "=== epoch:15, train acc:0.983, test acc:0.954 ===\n",
      "train loss:0.08173859733239168\n",
      "train loss:0.06854020700996175\n",
      "train loss:0.019081473426609027\n",
      "train loss:0.033837005761915895\n",
      "train loss:0.08080486580867188\n",
      "train loss:0.04711042524507383\n",
      "train loss:0.04059048075913257\n",
      "train loss:0.0237982747136359\n",
      "train loss:0.023166662041217798\n",
      "train loss:0.0483379317838585\n",
      "train loss:0.05063355212421353\n",
      "train loss:0.052136499705347686\n",
      "train loss:0.0272909044542574\n",
      "train loss:0.018453361349390664\n",
      "train loss:0.06999956317597426\n",
      "train loss:0.04909927339258151\n",
      "train loss:0.04496104019741137\n",
      "train loss:0.033071722791588154\n",
      "train loss:0.00975366582447705\n",
      "train loss:0.029524986729328524\n",
      "train loss:0.03485928852026752\n",
      "train loss:0.012324808576400774\n",
      "train loss:0.04163373474977442\n",
      "train loss:0.04092881090469278\n",
      "train loss:0.02139845886647955\n",
      "train loss:0.033794130576493935\n",
      "train loss:0.04837601940844662\n",
      "train loss:0.019841345836089923\n",
      "train loss:0.03734689975246147\n",
      "train loss:0.028709781894547377\n",
      "train loss:0.01673425126217565\n",
      "train loss:0.030808780606453628\n",
      "train loss:0.07145685590264593\n",
      "train loss:0.024470810056844212\n",
      "train loss:0.052357847923541895\n",
      "train loss:0.03468267613057147\n",
      "train loss:0.03982752950480865\n",
      "train loss:0.029134382307635896\n",
      "train loss:0.033804736360292235\n",
      "train loss:0.030387166844921862\n",
      "train loss:0.024908961366615242\n",
      "train loss:0.012955457145345503\n",
      "train loss:0.044229839179866094\n",
      "train loss:0.04342638580356191\n",
      "train loss:0.05948875971633967\n",
      "train loss:0.018132762381267032\n",
      "train loss:0.03074653496691336\n",
      "train loss:0.03366699787515905\n",
      "train loss:0.02404140428134406\n",
      "train loss:0.02604366382804032\n",
      "=== epoch:16, train acc:0.982, test acc:0.961 ===\n",
      "train loss:0.06560296055677103\n",
      "train loss:0.04092382604545757\n",
      "train loss:0.034396225929767085\n",
      "train loss:0.013767443404338482\n",
      "train loss:0.012343717606020805\n",
      "train loss:0.04440747979582027\n",
      "train loss:0.033936738665381404\n",
      "train loss:0.03231989110151698\n",
      "train loss:0.030041476438613112\n",
      "train loss:0.024710849005648988\n",
      "train loss:0.08230540197541338\n",
      "train loss:0.038066015532190904\n",
      "train loss:0.013502555610042781\n",
      "train loss:0.021000709260607944\n",
      "train loss:0.023827233306080064\n",
      "train loss:0.04230313549174359\n",
      "train loss:0.033173213051417766\n",
      "train loss:0.013188257522156655\n",
      "train loss:0.027541546941680473\n",
      "train loss:0.010906025590890737\n",
      "train loss:0.03340110219884574\n",
      "train loss:0.01621929169966748\n",
      "train loss:0.029008618477893617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.048105283354553004\n",
      "train loss:0.06524515309671243\n",
      "train loss:0.03186744300312961\n",
      "train loss:0.0186486021049591\n",
      "train loss:0.04720758530296946\n",
      "train loss:0.0389058493624036\n",
      "train loss:0.040778373345629504\n",
      "train loss:0.060703965752932236\n",
      "train loss:0.04326132173762675\n",
      "train loss:0.07567003534137436\n",
      "train loss:0.03170807929592922\n",
      "train loss:0.024541427466845116\n",
      "train loss:0.018648235473700033\n",
      "train loss:0.05585635243444839\n",
      "train loss:0.01572301589827211\n",
      "train loss:0.022384956278460915\n",
      "train loss:0.019160456634663516\n",
      "train loss:0.03902796863218932\n",
      "train loss:0.02240679044663645\n",
      "train loss:0.056353009234308714\n",
      "train loss:0.041396587528238375\n",
      "train loss:0.03206835046675038\n",
      "train loss:0.04506883864378084\n",
      "train loss:0.028763693507947097\n",
      "train loss:0.03457721831754987\n",
      "train loss:0.030210397945367486\n",
      "train loss:0.024011260804382144\n",
      "=== epoch:17, train acc:0.982, test acc:0.951 ===\n",
      "train loss:0.0448586439626553\n",
      "train loss:0.009991643753578382\n",
      "train loss:0.025196449750313734\n",
      "train loss:0.052198116389678695\n",
      "train loss:0.04605975094966446\n",
      "train loss:0.02895427494046441\n",
      "train loss:0.030292889816505967\n",
      "train loss:0.06271080881369405\n",
      "train loss:0.05074544804008835\n",
      "train loss:0.024076970381794575\n",
      "train loss:0.02498967748064594\n",
      "train loss:0.057557818662736154\n",
      "train loss:0.06462633011394812\n",
      "train loss:0.014082201810017065\n",
      "train loss:0.04063215057119373\n",
      "train loss:0.019382293649877043\n",
      "train loss:0.027273199262362408\n",
      "train loss:0.03779200481741305\n",
      "train loss:0.018111466030344244\n",
      "train loss:0.03947789993001512\n",
      "train loss:0.051207961342788814\n",
      "train loss:0.015140308061563372\n",
      "train loss:0.023577574035901994\n",
      "train loss:0.01846966374296108\n",
      "train loss:0.021333317921997802\n",
      "train loss:0.019748051719330858\n",
      "train loss:0.10453730988015479\n",
      "train loss:0.03771945105608864\n",
      "train loss:0.04835083074645416\n",
      "train loss:0.03957963268454573\n",
      "train loss:0.028546308667986957\n",
      "train loss:0.02749924502630111\n",
      "train loss:0.009770342859884547\n",
      "train loss:0.028768398187030208\n",
      "train loss:0.01923893716664109\n",
      "train loss:0.04946873978693642\n",
      "train loss:0.06539391069934314\n",
      "train loss:0.050657395753007914\n",
      "train loss:0.048720830272284424\n",
      "train loss:0.008999036329351067\n",
      "train loss:0.013829450686062499\n",
      "train loss:0.05123993392023621\n",
      "train loss:0.02828762328600578\n",
      "train loss:0.03342639889036941\n",
      "train loss:0.02991610495515094\n",
      "train loss:0.019037885727822664\n",
      "train loss:0.025758218452938405\n",
      "train loss:0.06549815948067608\n",
      "train loss:0.039345411011749806\n",
      "train loss:0.010583158975960336\n",
      "=== epoch:18, train acc:0.987, test acc:0.959 ===\n",
      "train loss:0.02851240063234634\n",
      "train loss:0.017150457565252922\n",
      "train loss:0.06547280390872207\n",
      "train loss:0.02163254139250434\n",
      "train loss:0.0075350551807244984\n",
      "train loss:0.03504842392619171\n",
      "train loss:0.02940411415795971\n",
      "train loss:0.03677425427332519\n",
      "train loss:0.017800604799018294\n",
      "train loss:0.02601350056912716\n",
      "train loss:0.013537697935694915\n",
      "train loss:0.033368701804241396\n",
      "train loss:0.01345121170041645\n",
      "train loss:0.03555417325789019\n",
      "train loss:0.020899839325311767\n",
      "train loss:0.02744765806117436\n",
      "train loss:0.023034159093230158\n",
      "train loss:0.03918631189726212\n",
      "train loss:0.02766667840748456\n",
      "train loss:0.03585432851521499\n",
      "train loss:0.07133906159187699\n",
      "train loss:0.04536654860411606\n",
      "train loss:0.03291744220294216\n",
      "train loss:0.015420589830097253\n",
      "train loss:0.025119411994445645\n",
      "train loss:0.018494404724394575\n",
      "train loss:0.05811585802571236\n",
      "train loss:0.016450864989149813\n",
      "train loss:0.02101927818424397\n",
      "train loss:0.016187648791646123\n",
      "train loss:0.02728614171443728\n",
      "train loss:0.026191513493475958\n",
      "train loss:0.012371289238944243\n",
      "train loss:0.01084714782851452\n",
      "train loss:0.009468081123572393\n",
      "train loss:0.03405059187808637\n",
      "train loss:0.046716777982991\n",
      "train loss:0.021080543437309383\n",
      "train loss:0.04242721963721521\n",
      "train loss:0.014190147329123938\n",
      "train loss:0.006019233247914876\n",
      "train loss:0.016390255299115915\n",
      "train loss:0.014769322587296064\n",
      "train loss:0.006684900310850862\n",
      "train loss:0.03515389832087157\n",
      "train loss:0.033062582303690635\n",
      "train loss:0.044936987110550684\n",
      "train loss:0.0357228669950635\n",
      "train loss:0.01706530532597246\n",
      "train loss:0.04670720528799704\n",
      "=== epoch:19, train acc:0.988, test acc:0.958 ===\n",
      "train loss:0.042581405257522825\n",
      "train loss:0.01400937580919176\n",
      "train loss:0.03834714904980872\n",
      "train loss:0.028389973311517824\n",
      "train loss:0.022946029259924002\n",
      "train loss:0.05602015161082971\n",
      "train loss:0.050479599995646086\n",
      "train loss:0.014802803814100405\n",
      "train loss:0.015131676474843269\n",
      "train loss:0.02519992378071152\n",
      "train loss:0.01679323233900678\n",
      "train loss:0.029593642095844877\n",
      "train loss:0.03715472919475304\n",
      "train loss:0.029547992934993324\n",
      "train loss:0.017451904217119455\n",
      "train loss:0.008250542472311995\n",
      "train loss:0.011601671605216825\n",
      "train loss:0.04035062920943935\n",
      "train loss:0.041124682663874285\n",
      "train loss:0.020443702524248403\n",
      "train loss:0.0193254726504928\n",
      "train loss:0.03314732258489407\n",
      "train loss:0.03479813314331466\n",
      "train loss:0.026227850590119936\n",
      "train loss:0.014867115090573717\n",
      "train loss:0.012767729864622374\n",
      "train loss:0.02072024326892952\n",
      "train loss:0.02560337918663263\n",
      "train loss:0.010221940493476462\n",
      "train loss:0.011257709795246849\n",
      "train loss:0.016468186709464986\n",
      "train loss:0.0081080667268161\n",
      "train loss:0.0703177960071571\n",
      "train loss:0.028768365199213098\n",
      "train loss:0.011445705510407588\n",
      "train loss:0.0133475008034882\n",
      "train loss:0.014593435680194451\n",
      "train loss:0.05959988720365212\n",
      "train loss:0.03551118928526778\n",
      "train loss:0.03514266835114104\n",
      "train loss:0.01621376517154134\n",
      "train loss:0.025055625237194245\n",
      "train loss:0.021614185841090844\n",
      "train loss:0.012893629854997906\n",
      "train loss:0.010300368011550109\n",
      "train loss:0.04629103247614168\n",
      "train loss:0.013274831241481011\n",
      "train loss:0.05944659818608823\n",
      "train loss:0.05054658048819049\n",
      "train loss:0.013998862269990007\n",
      "=== epoch:20, train acc:0.99, test acc:0.951 ===\n",
      "train loss:0.0434375391070801\n",
      "train loss:0.023611573064169687\n",
      "train loss:0.06696789044199605\n",
      "train loss:0.03138359998673107\n",
      "train loss:0.016614305727290456\n",
      "train loss:0.0537336657969813\n",
      "train loss:0.05858919532258421\n",
      "train loss:0.016065882263179974\n",
      "train loss:0.022128300049365213\n",
      "train loss:0.027136474304967254\n",
      "train loss:0.04756676266536896\n",
      "train loss:0.013860701276295885\n",
      "train loss:0.01560963472938881\n",
      "train loss:0.03494308174402233\n",
      "train loss:0.019197981871326476\n",
      "train loss:0.020528803966992602\n",
      "train loss:0.011636931990021637\n",
      "train loss:0.03710578126021565\n",
      "train loss:0.024403701891195194\n",
      "train loss:0.012105850096856526\n",
      "train loss:0.009492284154399969\n",
      "train loss:0.050433281407408354\n",
      "train loss:0.030562426638025955\n",
      "train loss:0.02562709999148482\n",
      "train loss:0.02895508813061301\n",
      "train loss:0.01144954496003181\n",
      "train loss:0.005978396144023488\n",
      "train loss:0.008799492911619288\n",
      "train loss:0.016551714710009588\n",
      "train loss:0.013013673802298701\n",
      "train loss:0.027604298057669868\n",
      "train loss:0.013930834411126698\n",
      "train loss:0.05313301455039169\n",
      "train loss:0.010778026791060309\n",
      "train loss:0.02791497551453441\n",
      "train loss:0.012551148763641124\n",
      "train loss:0.01499220510719679\n",
      "train loss:0.027694772003936618\n",
      "train loss:0.02523604387418392\n",
      "train loss:0.038348173945775256\n",
      "train loss:0.019594566972612474\n",
      "train loss:0.02365012098288267\n",
      "train loss:0.008607471564043305\n",
      "train loss:0.019251209912243494\n",
      "train loss:0.024520983462096337\n",
      "train loss:0.01953397368962154\n",
      "train loss:0.034104466656628536\n",
      "train loss:0.01450483739213716\n",
      "train loss:0.019127948164385476\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.959\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "                        \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Network Parameters!\n"
     ]
    }
   ],
   "source": [
    "# 매개변수 보존\n",
    "network.save_params(\"params.pkl\")\n",
    "print(\"Saved Network Parameters!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
