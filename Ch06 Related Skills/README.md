# Ch06 Related Skills

생성일: 2021년 1월 14일 오후 2:06
태그: 재이 김

## [6장 학습 관련 기술들]

6장에서는 신경망 학습에 중요한 기술 몇 가지를 공부했습니다. 먼저 가중치 매개변수를 갱신하여 최적값을 탐색하는 방법을 살펴봤는데, 확률적 경사 하강법(SGD)은 비등방성 함수에서의 탐색 경로가 비효율적이라는 단점을 가지고 있습니다. 따라서 이를 보완할 수 있는 방법인 모멘텀, AdaGrad, Adam 등을 살펴봤습니다. 

가중치의 초깃값은 신경망 학습에 아주 중요한 포인트입니다. 활성화함수로 ReLU를 사용할 때는 He 초깃값을, sigmoid나 tanh등의 S자 모양 곡선일 때는 Xavier 초깃값을 사용하는 게 권장됩니다.

배치 정규화는 각 층의 활성화값을 고르게 분포하기 위해 사용합는데, 이는 학습이 빨라지며, 가중치 초깃값에 크게 의존하지 않아도 된다는 장점을 가집니다.

마지막으로 오버피팅 문제를 보완하는 기술로는 가중치 감소와 드롭아웃 기법이 대표적입니다. 그리고 하이퍼파라미터 최적화를 MNIST 데이터셋을 이용해 구현했습니다.

## [정리]

- 매개변수 갱신 방법에는 확률적 경사 하강법(SGD) 외에도 모멘텀, AdaGrad, Adam 등이 있다.
- 가중치 초깃값을 정하는 방법은 올바른 학습을 하는 데 매우 중요하다.
- 가중치의 초깃값으로는 'Xavier 초깃값'과 'He 초깃값'이 효과적이다.
- 배치 정규화를 이용하면 학습을 빠르게 진행할 수 있으며, 초깃값에 영향을 덜 받게 된다.
- 오버파이팅을 억제하는 정규화 기술로는 가중치 감소와 드롭아웃이 있다.
- 하이퍼파라미터 값 탐색은 최적값이 존재할 법한 범위를 점차 좁히면서 하는 것이 효과적이다.

## [파일 설명]

optimizer_compare_naive : SGD, Momentum, AdaGrad, Adam의 학습 패턴을 비교합니다.

optimizer_compare_mnist : SGD, Momentum, AdaGrad, Adam의 학습 속도를 비교합니다.

weight_init_activation_histogram : 은닉층의 활성화값 분포를 히스토그램으로 확인합니다.

weight_init_compare : 가중치의 초깃값에 따른 학습 속도를 비교합니다.

batch_norm_test : 배치 정규화의 효과를 학습 속도로 확인합니다.

overfit_weight_decay : 일부러 오버피팅을 일으킨 후 훈련 데이터와 시험 데이터를 비교합니다.

overfit_dropout : 드롭아웃의 유뮤에 따른 오버피팅 정도를 확인합니다.

hyperparameter_optimization : 하이퍼파라미터를 최적화하는 코드입니다.

## [심화]

배치 정규화란?

- 각 층의 활성화 함수의 출력값 분포가 **골고루 분포되도록 강제**하는 방법으로, 각 층에서의 활성화 함수 출력값이 정규분포를 이루도록 하는 방법이다.
- 보통의 신경망 모델에서는 데이터 입력 전 전체 데이터 세트를 정규화하는 것이 일반적, 배치 정규화는 미니 배치별로 학습 직전(후)에 데이터를 정규화하는 방식
- 배치 정규화의 장점: 학습 속도 개선, 초깃값에 의존하지 않아도 됨, 오버피팅을 억제, 기울기 소실 문제 개선
- 배치 단위로 학습했을 때의 문제: 내부 공변량의 변화 (Internal Covariant Shift)

### 1. 내부 공변량 변화와 배치 정규화

- 내부 공변량 변화 (Internal Covariant Shift): 학습 과정에서 계층 별로 입력의 데이터 분포가 달라지는 현상 (학습하는 동안 이전 층에서의 가중치 매개변수가 변함에 따라, 활성화 함수 출력값의 분포도 변화하는 경우)

 배치 단위로 학습을 하면 각 층에서 입력으로 받게 되는 feature가 연산을 거친 뒤 활성화 함수에 적용되는데, 이는 연산 전후에 데이터 분포가 달라질 수 있다는 문제점을 가진다. 즉, 배치 간의 데이터가 상이하다고 말할 수 있다.

![Ch06%20Related%20Skills%2035fc8372f9054bfdbcd602222f91f20b/Untitled.png](Ch06%20Related%20Skills%2035fc8372f9054bfdbcd602222f91f20b/Untitled.png)

![Ch06%20Related%20Skills%2035fc8372f9054bfdbcd602222f91f20b/Untitled%201.png](Ch06%20Related%20Skills%2035fc8372f9054bfdbcd602222f91f20b/Untitled%201.png)

 배치 정규화는 이 문제를 해결할 수 있다.  다음 그림을 보면 배치 단위나 layer에 ㄸ라서 입력값의 분포가 모두 다르지만, 정규화를 통해 분포를 평균 0, 표준편차 1로 조정한다.

![Ch06%20Related%20Skills%2035fc8372f9054bfdbcd602222f91f20b/Untitled%202.png](Ch06%20Related%20Skills%2035fc8372f9054bfdbcd602222f91f20b/Untitled%202.png)

- **결론**: 배치 단위로 학습할 때의 문제점인 내부 공변량 변화는 배치 정규화에서는 해결 가능하다.

### 2. 배치 정규화 알고리즘 상세 설명

![Ch06%20Related%20Skills%2035fc8372f9054bfdbcd602222f91f20b/Untitled%203.png](Ch06%20Related%20Skills%2035fc8372f9054bfdbcd602222f91f20b/Untitled%203.png)

### 3. 배치 정규화의 한계

**1. 미니배치의 크기에 의존적이다**

- 배치 정규화는 너무 작은 배치 크기에서는 잘 동작하지 않을 수 있습니다.  단적으로 배치 크기를 1로 하게 되면 분산은 0이 됩니다. 이처럼 작은 미니 배치에서는 배치 정규화의 효과가 극단적으로 작용되어 훈련에 악영향을 줄 수 있습니다.

**2. RNN에 적용하기 어렵다**

- RNN이란? '기억'을 가지고 시퀀스 데이터를 처리할 수 있는 순환신경망입니다. 새로운 입력이 들어올때마다 네트워크는 자신의 기억을 조금씩 수정합니다. 결국 입력을 모두 처리하고 난 후 네트워크에게 남겨진 기억은 시퀀스 전체를 요약하는 정보가 됩니다.
- RNN은 각 시점마다 다른 통계치를 가집니다. 이는 RNN에 배치 정규화를 적용하는 것을 힘들게 만듭니다. 이 문제는 층 정규화(Layer Normalization)으로 해결 가능합니다.

**3. 결론**: 배치 정규화는 항상 효과적이지는 않다. 상황에 따라 적절한 방법을 사용해야 한다.

### 참고 사이트 및 이미지 출처

[[딥러닝II] 7강. 배치 정규화](https://www.youtube.com/watch?v=iaweeYJP4WU)

[위키독스](https://wikidocs.net/61375)

[배치 정규화(Batch Normalization)](https://gaussian37.github.io/dl-concept-batchnorm/)

[[딥러닝 기본] Deep Learning 학습최적화 개선](https://warm-uk.tistory.com/52)